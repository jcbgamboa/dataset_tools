# Tools for the _Customer Reviews Dataset_

The tools here are made to process the dataset available in
https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html
(specifically, the annotated 5 Products dataset)

#Available tools:

## `generate_folds.sh`

Given the dataset containing all the 5 product files in a folder, the
script generates 4 complete datasets, each containing 5 folds. The
4 datasets differ in that the `sources.txt` files contain different
types of data.

 * `non_lemmatized_text`: Text sequences of words, one in each line.
 * `lemmatized_text`:  Same as `non_lemmatized_text`, but words are
	lemmatized. This should be useful to reduce the vocabulary
	size.
 * `non_lemmatized_trees`: Flattened dependency trees (the output of
	`create_vocabulary.py`), one in each line.
 * `lemmatized_trees`: Same as `non_lemmatized_trees`, but words are
	lemmatized.

Each of the 4 datasets is composed by 5 folders. In each folder, the
preprocessed input corresponding to 4 products composes the
training set, and the last product composes the test set. The goal is
to perform a 5-fold cross-validation.


## `parse_dataset.sh`

Runs `generate_sequences.py` each of the products and outputs the
result in a folder. This is used by `generate_folds.sh` to produce
the final datasets.


## `generate_sequences.py`

Generates two output files. The first of them contains, for each line
in the file (containing the aspects, the polarity of the aspects, and
the sentence where the aspects appear), only the sentence, with words
separated by space. Review titles are ignored (since they don't
contain aspects). For example, the first three lines of the output of
for the `Canon G3.txt` file would be:

```
i recently purchased the canon powershot g3 and am extremely satisfied with the purchase . 
the camera is very easy to use , in fact on a recent trip this past week i was asked to take a picture of a vacationing elderly group . 
after i took their picture with their camera , they offered to take a picture of us . 
```

The second file generated by `generate_sequences.py` is composed
of BIO tags. B tags indicate that a word is the beginning of an
aspect. I tags indicate that a word is the continuation of an aspect
(i.e., I tags have to always follow B tags). O tags indicate that the
word is not part of any aspect. (see
[this Stack Overflow question](https://stackoverflow.com/questions/17116446/what-do-the-bilou-tags-mean-in-named-entity-recognition)
for a similar tagging scheme)

For example, for those three lines shown above, the result would be:

```
O O O O B-A I-A I-A O O O O O O O O
O O O O O O B-A O O O O O O O O O O O O O O O O O O O O O O O
O O O O O O O O O O O O O O O O O O
```

Because sometimes the aspect name is a little different from whatever
is written in the sentence (e.g., the review may say "useful features",
but the aspect is actually "feature"), the script will check for the
lemma of the words. It will also try to separate words connected by
hyphens (for similar reasons).


## `create_vocabulary.py`

Given a file, counts all the occurrences of the words in the file, and
outputs the words that occur at least `--min_count` times.


## `generate_parse_trees.py`

Code to generate parsed dependency trees from a given code. The main
function to be called is `parse()`. It is used by
`generate_sequences.py` when the option `--generate_parse_trees`
is used.




